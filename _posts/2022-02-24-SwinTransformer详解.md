---
redirect_from: /_posts/2022-02-24-SwinTransformerè¯¦è§£.md/
title: SwinTransformerè¯¦è§£
tags:
  - Python
  - AI
  - ComputerVision
  - transformer
---
# Swin Transformer[<sup>1</sup>](#swin-transformer-1)

## V1

<div id="top"/>

### å‰è¨€

**æ›´å¥½åœ°å°†è¯­è¨€æ¨¡å‹å»ºæ¨¡åˆ°è§†è§‰**æ˜¯è¿‘å‡ å¹´çš„ä¸€å¤§çƒ­ç‚¹ï¼Œ**ViT**[<sup>2</sup>](#vit)ï¼Œ**DETR**[<sup>3</sup>](#detr)ç­‰æ¨¡å‹å°†Transformerå¼•å…¥è§†è§‰é¢†åŸŸå¹¶å–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚ViTç›´æ¥æŠŠå›¾ç‰‡åˆ’åˆ†æˆpatchï¼ŒæŠŠpatchçœ‹ä½œwordï¼Œå°†å›¾ç‰‡å»ºæ¨¡æˆsentenceï¼›DETRå€ŸåŠ©CNNæå–ç‰¹å¾ï¼Œå°†Transformerå½“ä½œneckã€‚

è€Œç›®å‰Transformeråº”ç”¨äºå›¾åƒé¢†åŸŸçš„ä¸¤å¤§æŒ‘æˆ˜ï¼š

- è§†è§‰å®ä½“å˜åŒ–å¤§ï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹è§†è§‰Transformeræ€§èƒ½æœªå¿…å¾ˆå¥½
- å›¾åƒåˆ†è¾¨ç‡é«˜ï¼Œåƒç´ ç‚¹å¤šï¼ŒåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„Transformerçš„è®¡ç®—å¤æ‚åº¦æ˜¯tokenæ•°é‡çš„å¹³æ–¹[<sup>4</sup>](#transformer-complexity)ï¼Œè‹¥å°†æ¯ä¸ªåƒç´ ç‚¹ä½œä¸ºä¸€ä¸ªtokenï¼Œå…¶è®¡ç®—é‡å·¨å¤§

ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå‡ºç°äº†åŒ…å«**æ»‘çª—æ“ä½œ**ï¼Œå…·æœ‰**å±‚çº§è®¾è®¡**åŠçš„Swin Transformer

### è§£å†³æ–¹æ³•

1. ä½¿ç”¨åˆ†å±‚ç»“æ„ï¼Œä½¿å¾—æ¨¡å‹èƒ½çµæ´»å¤„ç†ä¸åŒå°ºåº¦çš„å›¾ç‰‡ã€‚

   ![image-20220224191303395](https://s2.loli.net/2022/02/24/6tVnJGmuqQ9aFwO.png)

   

2. ä½¿ç”¨window self-attentionï¼Œé™ä½è®¡ç®—å¤æ‚åº¦

   ç®€å•åœ°è¯´ï¼Œä¼ ç»ŸTransformeréƒ½æ˜¯åŸºäºå…¨å±€æ¥è®¡ç®—æ³¨æ„åŠ›çš„ï¼Œè€ŒSwin Transformerå°†æ³¨æ„åŠ›è®¡ç®—é™åˆ¶åœ¨æ¯ä¸ªwindowé‡Œï¼Œè¿›è€Œå‡å°‘è®¡ç®—é‡ã€‚

   

   ä¸¾ä¸ªä¾‹å­ï¼Œè®¾ä¸€å¼ å›¾ç‰‡å…±æœ‰$h*w$ä¸ªpatchesï¼Œæ¯ä¸ªçª—å£åŒ…æ‹¬$n*n$ä¸ªpatchesã€‚

   é‚£ä¹ˆä¼ ç»ŸTransformerçš„self-attentionè®¡ç®—å¤æ‚åº¦ä¸º${hw}^2$

   è€ŒSwin Transformerè®¡ç®—å¤æ‚åº¦ä¸ºçª—å£è®¡ç®—å¤æ‚åº¦\*çª—å£æ•°é‡

   çª—å£è®¡ç®—å¤æ‚åº¦ä¸º${(n^2)}^2$, çª—å£æ•°é‡ä¸º$\frac {h*w}{n^2}$

   æ•…Swin Transformerè®¡ç®—å¤æ‚åº¦ä¸º$\frac{h*w}{n^2}*(n^2)^2=h*w*n^2$ (å…¶ä¸­$n^2$æ˜¯æ¯ä¸ªçª—å£åŒ…å«çš„patchesæ•°ï¼Œæ˜¯ç¡®å®šå€¼)

   

ä¸‹é¢å¼€å§‹è¯¦ç»†çœ‹çœ‹ï¼š

### æ•´ä½“ç»“æ„

![image-20220224193823861](https://s2.loli.net/2022/02/24/HJLlIS6fURaX39T.png)

ä¸€å…±4ä¸ªstageï¼Œæ¯ä¸ªstageç”±Patch Merging å’Œ Swin Transformer Blockç»„æˆï¼Œé€stageç¼©å°ç‰¹å¾å›¾åˆ†è¾¨ç‡ï¼ŒåƒCNNä¸€æ ·æ‰©å¤§æ„Ÿå—é‡ã€‚

æºç å®ç°é€»è¾‘å¦‚ä¸‹å›¾[<sup>5</sup>](#pic)

![image-20220224195008866](https://s2.loli.net/2022/02/24/LY1o4FXHmZTwg7x.png)

- é¦–å…ˆåšäº†ä¸€ä¸ªPatch Embeddingï¼Œå°†å›¾ç‰‡åˆ‡æˆä¸€ä¸ªä¸ªå›¾å—å„¿ï¼Œå¹¶åµŒå…¥åˆ°embedding
- ä¸ViTä¸åŒçš„æ˜¯ï¼Œåœ¨è¾“å…¥æ—¶ç»™embeddingè¿›è¡Œç»å¯¹ä½ç½®ç¼–ç åœ¨Swin Transformerä¸­ä½œä¸ºä¸€ä¸ªå¯é€‰é¡¹ï¼Œå¹¶ä¸”åœ¨è®¡ç®—Attentionæ—¶åšäº†ä¸€ä¸ªç›¸å¯¹ä½ç½®ç¼–ç 
- æ¯ä¸ªstageç”±å¤šä¸ªblockå’ŒPatch Mergingç»„æˆ
- Blockå…·ä½“ç»“æ„å¦‚ä¸Šä¸Šå›¾ä¸­çš„bï¼Œä¸»è¦ç”±LN ã€MLPã€Window Attentionå’ŒShifted Window Attentionç»„æˆ
- ViTå•ç‹¬åŠ å…¥ä¸€ä¸ªå¯å­¦ä¹ å‚æ•°ä½œä¸ºåˆ†ç±»çš„å‚æ•°ï¼Œè€ŒSwin Transformerç›´æ¥å¹³å‡æ± åŒ–ï¼Œè¾“å‡ºåˆ†ç±»

```python
class SwinTransformer(nn.Module):

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        #  åœ¨è¿™é‡Œåšäº†ä¸€ä¸ªpatch embeddingï¼Œå°†å›¾ç‰‡åˆ‡å‰²æˆä¸é‡å çš„patchesï¼Œå¹¶ä¸”åµŒå…¥embedding
        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # è¿™é‡Œåœ¨è¾“å…¥æ—¶ç»™embeddingè¿›è¡Œä½ç½®ç¼–ç ä½œä¸ºä¸€ä¸ªå¯é€‰é¡¹
        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # Blockå’ŒPatchMergigng
        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            # BasicLayerä½œä¸ºç¨‹åºä¸­çš„stageï¼Œé¡ºåºä¸è®ºæ–‡ç¨æœ‰ä¸åŒ
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],# éšæœºæ·±åº¦è¡°å‡
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,# PatchMerging
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1) # è¿™é‡Œä½¿ç”¨å¹³å‡æ± åŒ–ï¼Œè¾“å‡ºåˆ†ç±»
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x
    
    
    

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)
        flops += self.num_features * self.num_classes
        return flops
```

### Patch Embedding

å¯¹è¾“å…¥å›¾ç‰‡è¿›å…¥Blockå‰çš„å¤„ç†ï¼Œå°†å›¾ç‰‡åˆ‡æˆä¸€ä¸ªä¸ª$patch\_size * patch\_size*channels$å¤§å°çš„patchesï¼Œç„¶ååµŒå…¥å‘é‡ï¼ˆreshapeæˆ$n*(patch\_size^2*channels)\ where\ n=\frac{h*w}{patch\_size^2}$çš„patcheså—å„¿, å†é€šè¿‡çº¿æ€§å˜æ¢å°†patchesæŠ•å½±åˆ°ç»´åº¦ä¸ºDçš„ç©ºé—´ä¸Š, ä¹Ÿå°±æ˜¯ç›´æ¥å°†åŸæ¥å¤§å°ä¸º$h*w*channels$çš„äºŒç»´å›¾åƒå±•å¹³æˆnä¸ªå¤§å°ä¸º$patch\_size^2*channels$çš„ä¸€ç»´å‘é‡)

å¯ä»¥é€šè¿‡äºŒç»´å·ç§¯å±‚ï¼Œå°†strideï¼Œkernel sizeè®¾ç½®ä¸ºpatch_sizeå¤§å°ã€‚

è¿™é‡Œçš„å¤„ç†å’ŒViTåŸºæœ¬æ˜¯ä¸€æ ·çš„ã€‚

å·ç§¯è¾“å‡ºè®¡ç®—å…¬å¼ï¼š$\frac{n+2p-f}{s}+1$

 å°†å›¾åƒå®½é«˜å¸¦å…¥åˆ†åˆ«å¾—åˆ°

$\frac{h+0-p}{p}+1=\frac{h}{p}$, $\frac{w+0-p}{p}+1=\frac{w}{p}$

äºŒè€…ç›¸ä¹˜å³$\frac{h*w}{p^2}=n$, è¿™å°±ç­‰ä»·äºå°†è¾“å…¥å›¾åƒåˆ’åˆ†æˆnä¸ªå¤§å°ä¸º$p^2*c$ çš„patches

```python
class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding
    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size) # -> (img_size, img_size)
        patch_size = to_2tuple(patch_size) # -> (patch_size, patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops
```

### Patch Merging

åœ¨æ¯ä¸ªstageå¼€å§‹å‰åšé™é‡‡æ ·ï¼Œç”¨äºç¼©å°åˆ†è¾¨ç‡ï¼Œè°ƒæ•´é€šé“æ•°ï¼Œå½¢æˆå±‚æ¬¡åŒ–è®¾è®¡ï¼Œä¸€å®šç¨‹åº¦èŠ‚çœç®—åŠ›ã€‚

è€ŒCNNåˆ™ä½¿ç”¨stride=2çš„å·ç§¯æˆ–æ± åŒ–æ¥é™é‡‡æ ·ã€‚

```python
class PatchMerging(nn.Module):
    r""" Patch Merging Layer.
    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        # 0::2 å¶æ•°
        # 1::2 å¥‡æ•°
        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x) #  4ç»´åˆ°2ç»´çš„çº¿æ€§å±‚ï¼Œè¾¾åˆ°åŸpatché€šé“c->2cï¼Œåˆ†è¾¨ç‡å‡åŠé€šé“æ•°åŠ å€çš„æ•ˆæœã€‚

        return x

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        return flops
```

![img](https://s2.loli.net/2022/02/26/g8koARqVYHIWNcZ.jpg)

### Basic Layer

ç¨‹åºå®ç°çš„stage

```python
class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.
    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth # swin transformer blocks çš„ä¸ªæ•°
        self.use_checkpoint = use_checkpoint

        # ä»0å¼€å§‹å¶æ•°ä½ç½®blockè®¡ç®—çš„æ˜¯W-MSAï¼›å¥‡æ•°ä½ç½®blockè®¡ç®—çš„æ˜¯SW-MSAï¼Œä¸”shift_size = window_size // 2
        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops
```

### Window Partition/Reverse

`window partition`å‡½æ•°æ˜¯ç”¨äºå¯¹å¼ é‡åˆ’åˆ†çª—å£ï¼ŒæŒ‡å®šçª—å£å¤§å°ã€‚å°†åŸæœ¬çš„å¼ é‡ä» `N H W C`, åˆ’åˆ†æˆ `num_windows*B, window_size, window_size, C`ï¼Œå…¶ä¸­ `num_windows = H*W / window_size`ï¼Œå³çª—å£çš„ä¸ªæ•°ã€‚è€Œ`window reverse`å‡½æ•°åˆ™æ˜¯å¯¹åº”çš„é€†è¿‡ç¨‹ã€‚è¿™ä¸¤ä¸ªå‡½æ•°ä¼šåœ¨åé¢çš„`Block`å’Œ`Window Attention`ç”¨åˆ°ã€‚

```python
def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x
```



### Swin Transformer Block

è¿™éƒ¨åˆ†æ˜¯ç¨‹åºçš„æ ¸å¿ƒï¼Œæ¶‰åŠåˆ°ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œmaskï¼Œwindow self-attention ï¼Œshifted window self-attention

å…ˆæ¥çœ‹çœ‹Swin Transformer Blockçš„æ•´ä½“ï¼Œå†ä»”ç»†çœ‹çœ‹å…¶ä»–æ¨¡å—

Swin Transformer Blockç»“æ„å›¾ï¼š

![image-20220226155505740](https://s2.loli.net/2022/02/26/Gi7eFUbBHrACsqa.png)

Swin Transformerä½¿ç”¨window self-attention é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä¸ºä¿è¯ä¸é‡å çª—å£ç›´æ¥æœ‰è”ç³»ï¼Œä½¿ç”¨shifted window self-attentioné‡æ–°è®¡ç®—äº†ä¸€éçª—å£åç§»åçš„è‡ªæ³¨æ„åŠ›ï¼Œæ‰€ä»¥Swin Transformer Blockéƒ½æ˜¯æˆå¯¹å‡ºç°çš„ï¼Œå³W-MSAå’ŒSW-MASAä¸ºä¸€å¯¹ï¼Œä¸åŒå¤§å°çš„blockä¸ªæ•°ä¹Ÿéƒ½æ˜¯å¶æ•°ï¼Œä¸å¯èƒ½ä¸ºå¥‡æ•°ã€‚

æ•´ä¸ªBlockçš„æµç¨‹æ˜¯ï¼š

- å¯¹ç‰¹å¾å›¾è¿›è¡ŒLayerNormï¼›

- æ ¹æ®`self.shift_size`å†³å®šæ˜¯å¦éœ€è¦å¯¹ç‰¹å¾å›¾è¿›è¡Œshiftï¼›

- å°†ç‰¹å¾å›¾åˆ‡æˆä¸€ä¸ªä¸ªwindowï¼›

- è®¡ç®—Attentionï¼Œ é€šè¿‡`self.attn_mask`æ¥åŒºåˆ†æ˜¯Window Attentionè¿˜æ˜¯ Shifted Window Attentionï¼›

- å°†å„ä¸ªçª—å£åˆèµ·æ¥ï¼›
- å¦‚æœæœ‰shiftæ“ä½œï¼Œreverse shiftæ¢å¤ï¼›
- dropoutå’Œæ®‹å·®è¿æ¥
- å†ç»è¿‡ä¸€å±‚LN+çº¿æ€§å±‚ï¼Œdropoutå’Œæ®‹å·®è¿æ¥

```python
class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        # è¿™æ˜¯mask attentionçš„ç›¸å…³ä»£ç 
        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)  # LN
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:   # æ ¹æ®self.shift_sizeå†³å®šæ˜¯å¦éœ€è¦å¯¹ç‰¹å¾å›¾è¿›è¡Œshift
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows å°†ç‰¹å¾å›¾åˆ‡æˆä¸€ä¸ªä¸ªwindow
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(se
                                        lf.norm2(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops
```

### Window Attention

å‰é¢ä¹Ÿæåˆ°å¹¶ä¸¾ä¾‹è¯´æ˜äº†ï¼ŒSwin Transformerç›¸è¾ƒäºä¼ ç»ŸTransformerçš„å…¨å±€è®¡ç®—æ³¨æ„åŠ›ï¼Œå®ƒå°†æ³¨æ„åŠ›çš„è®¡ç®—é™åˆ¶åœ¨æ¯ä¸ªçª—å£å†…ï¼Œå¤§å¹…å‡å°‘è®¡ç®—é‡ã€‚

åŸå§‹å…¬å¼[<sup>6</sup>](#origin_formula)ï¼š

$Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d}})V$

Swin Transformerå…¬å¼ï¼š

$Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d}}+B)V$

å¯ä»¥çœ‹åˆ°å…¬å¼ä¸Šçš„åŒºåˆ«åªæ˜¯åŠ äº†ä¸€ä¸ªç›¸å¯¹ä½ç½®ç¼–ç 

```python
class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads #  æ¯ä¸ªæ³¨æ„åŠ›å¤´å¯¹åº”çš„é€šé“æ•°
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH
            # è®¾ç½®ä¸€ä¸ªè¿™æ ·å½¢çŠ¶çš„å¯å­¦ä¹ å‚æ•°ç”¨äºåç»­çš„ä½ç½®ç¼–ç 

        # ç›¸å¯¹ä½ç½®ç¼–ç  ğŸ‘‡
        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)
		# ç›¸å¯¹ä½ç½®ç¼–ç  ğŸ‘†
        
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        # å¯¹qä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾ç³»æ•°ç„¶åä¸kç›¸ä¹˜ï¼Œå¾—åˆ°attnå¼ é‡
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        # æ ¹æ®ç›¸å¯¹ä½ç½®ç¼–ç ç´¢å¼•é€‰å–ï¼Œå¾—åˆ°ä¸€ä¸ªç¼–ç å‘é‡åŠ åˆ°attnä¸Š
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        # å¯¹åº”Swin Transformer Blockä¸­çš„SW-MSA
        # ä¸è€ƒè™‘maskçš„æƒ…å†µå°±æ˜¯å’Œtransformerä¸€æ ·çš„softmaxï¼Œdropoutï¼Œä¸VçŸ©é˜µç›¸ä¹˜å†ç»è¿‡ä¸€å±‚å…¨è¿æ¥å±‚å’Œdropout
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops
```

ç›¸å¯¹ä½ç½®ç¼–ç ï¼š

é¦–å…ˆQKè®¡ç®—å‡ºæ¥çš„çš„Attentionå¼ é‡å½¢çŠ¶ä¸º`(numWindows*B, num_heads, window_size*window_size,window_size*window_size)`

è€Œå¯¹äºAttentionå¼ é‡æ¥è¯´ï¼Œ**ä»¥ä¸åŒå…ƒç´ ä¸ºåŸç‚¹å…¶ä»–å…ƒç´ åæ ‡æ˜¯ä¸åŒçš„**ï¼Œå¦‚å›¾ï¼Œwindow_size=2ä¸ºä¾‹

![image-20220226185803729](https://s2.loli.net/2022/02/26/UmDL3lY9hov2g8i.png)

é¦–å…ˆç”¨torch.arangeå’Œtorch.meshgridç”Ÿæˆå¯¹åº”åæ ‡, ä»¥window_size=2ä¸ºä¾‹

```python
coords_h = torch.arange(self.window_size[0])
coords_w = torch.arange(self.window_size[1])
coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
"""
   (tensor([[0, 0],
           [1, 1]]), 
   tensor([[0, 1],
           [0, 1]]))
"""
```

å †å èµ·æ¥å±•å¼€ä¸ºä¸€ä¸ªäºŒç»´å‘é‡

```python
coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
"""
tensor([[0, 0, 1, 1],
        [0, 1, 0, 1]])
"""
```

ä½¿ç”¨å¹¿æ’­æœºåˆ¶ï¼Œåˆ†åˆ«åœ¨ç¬¬ä¸€ç»´ï¼Œç¬¬äºŒç»´ï¼Œæ’å…¥ä¸€ä¸ªç»´åº¦ï¼Œè¿›è¡Œå¹¿æ’­ç›¸å‡

```python
relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  
#                     (2, Wh*Ww, 1)              (2, 1, Wh*Ww)
# (2, Wh*Ww, Wh*Ww)
```

å¾—åˆ° (2, Wh\*Ww, Wh\*Ww)çš„å¼ é‡

åŠ ä¸Šåç§»é‡ï¼Œè®©ç´¢å¼•ä»0å¼€å§‹

```python
relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
relative_coords[:, :, 1] += self.window_size[1] - 1
```

å¯¹äº(1, 2)å’Œ(2, 1)è¿™ç»„åæ ‡ï¼Œåœ¨äºŒç»´ä¸Šæ˜¯ä¸åŒçš„ï¼Œä½†ç»è¿‡x, yåæ ‡ç›¸åŠ è½¬æ¢ä¸ºä¸€ç»´åç§»æ—¶æ˜¯ä¸å¯åŒºåˆ†çš„ï¼Œæ•…ä½¿ç”¨ä¸€ä¸ªä¹˜æ³•æ“ä½œè¿›è¡ŒåŒºåˆ†

```python
relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
```

å†åœ¨æœ€åä¸€ç»´ä¸Šè¿›è¡Œæ±‚å’Œï¼Œå±•å¼€æˆä¸€ä¸ªä¸€ç»´åæ ‡ï¼Œå¹¶æ³¨å†Œä¸ºä¸€ä¸ªä¸å‚ä¸ç½‘ç»œå­¦ä¹ çš„å˜é‡

```python
relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
self.register_buffer("relative_position_index", relative_position_index)
```

![image-20220226201727119](https://s2.loli.net/2022/02/26/vrkTJU9Lc8jxFX6.png)

åé¢è®¡ç®—QKå€¼æ—¶ï¼Œæ¯ä¸ªQKå€¼éƒ½è¦åŠ ä¸Šå¯¹åº”çš„ç›¸å¯¹ä½ç½®ç¼–ç 

è¿™ä¸ªrelative_position_indexçš„ä½œç”¨å°±æ˜¯æ ¹æ®æœ€ç»ˆç´¢å¼•å€¼æ‰¾åˆ°å¯¹åº”å¯å­¦ä¹ çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œä»¥M=2çš„çª—å£ä¸ºä¾‹ï¼Œè¿™ä¸ªå€¼çš„æ•°å€¼èŒƒå›´æ˜¯[0,8], æ‰€ä»¥ç›¸å¯¹ä½ç½®ç¼–ç å¯ä»¥ç”±3\*3çš„çŸ©é˜µè¡¨ç¤ºï¼Œæ¨å¹¿è‡³çª—å£å¤§å°ä¸ºMå³$(2M-1)*(2M-1)$çš„çŸ©é˜µã€‚

![image-20220226201654464](https://s2.loli.net/2022/02/26/3xSjymbgcnNBq7V.png)

ç»§ç»­ä»¥window_size=2ä¸ºä¾‹ï¼Œå³M=2ï¼Œè®¡ç®—ä½ç½®1å¯¹åº”çš„$M^2$ä¸ªQKå€¼æ—¶ï¼Œåº”ç”¨çš„relative_position_index = [ 4, 5, 7, 8]ï¼Œï¼ˆ$M^2$ä¸ªï¼Œè¿™é‡Œæ˜¯4ä¸ªï¼‰å¯¹åº”çš„æ•°æ®å°±æ˜¯ä¸Šå›¾ç´¢å¼•4ï¼Œ5ï¼Œ7ï¼Œ8ä½ç½®å¯¹åº”çš„$M^2$ç»´æ•°æ®ï¼Œ$relative\_position.shape=(M^2*M^2)$

### Shifted Window Attention

å‰é¢çš„Window Attentionæ˜¯åœ¨æ¯ä¸ªçª—å£ä¸‹è®¡ç®—æ³¨æ„åŠ›çš„ï¼Œä¸ºäº†æ›´å¥½åœ°ä¸å…¶ä»–windowå»ºç«‹è”ç³»ï¼ŒSwin Transformerå¼•å…¥äº†shifted windowæ“ä½œ

![image-20220226205655202](https://s2.loli.net/2022/02/26/ZJi1uCdQUVcxNrI.png)

å·¦è¾¹æ˜¯æ²¡æœ‰é‡å çª—å£çš„Window Attentionï¼Œå³è¾¹æ˜¯å°†çª—å£è¿›è¡Œç§»ä½çš„Shift Window Attentionï¼Œç§»ä½åçš„çª—å£å¯ä»¥åŒ…å«åŸæœ¬ç›¸é‚»çª—å£çš„å…ƒç´ ï¼Œä½†ä¹Ÿå¼•å…¥äº†ä¸€ä¸ªé—®é¢˜ï¼Œå³windowçš„ä¸ªæ•°ç”±åŸæ¥çš„4ä¸ªå˜ä¸ºäº†9ä¸ªï¼ˆä»¥ä¸Šå›¾ä¸ºä¾‹ï¼‰

åœ¨å®é™…ä»£ç ä¸­ï¼Œä»–ä»¬æ˜¯é€šè¿‡å¯¹ç‰¹å¾å›¾ç§»ä½ï¼Œå¹¶ç»™Attentionè®¾ç½®maskæ¥å®ç°ã€‚èƒ½åœ¨ä¿æŒåŸæœ‰windowä¸ªæ•°ä¸‹ï¼Œæœ€åè®¡ç®—ç»“æœç­‰ä»·ã€‚

![image-20220226210120061](https://s2.loli.net/2022/02/26/XTCsKAGeR729Yrv.png)

### ç‰¹å¾å›¾ç§»ä½

ä»£ç é‡Œä½¿ç”¨torch.rollæ¥å®ç°

`roll(x, shifts=-1, dims=1)`    `roll(x, shifts=-1, dims=2)`

![image-20220226210519046](https://s2.loli.net/2022/02/26/QGNALEq7gbSnFkz.png)

å¦‚æœéœ€è¦`reverse cyclic shift`çš„è¯åªéœ€æŠŠå‚æ•°`shifts`è®¾ç½®ä¸ºå¯¹åº”çš„æ­£æ•°å€¼ã€‚

### Attention Mask

é€šè¿‡è®¾ç½®åˆç†çš„maskå¯ä»¥è®©Shifted Window Attentionåœ¨ä¸Window Attentionç›¸åŒçª—å£çš„ä¸ªæ•°ä¸‹è¾¾åˆ°ç­‰ä»·çš„è®¡ç®—æ•ˆæœã€‚

å¦‚ä¸Šå›¾ï¼Œåœ¨(5,3)\(7,1)\(8,6,2,0)ç»„æˆçš„æ–°çª—å£ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è®¡ç®—Attentionçš„æ—¶å€™ï¼Œè®©å…·ç›¸åŒindexçš„QKè¿›è¡Œè®¡ç®—ï¼Œè€Œå¿½ç•¥ä¸åŒindexçš„QKè®¡ç®—ç»“æœã€‚æ ¹æ®è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å…¬å¼ï¼Œæœ€åéœ€è¦è¿›è¡ŒSoftmaxæ“ä½œï¼Œå¦‚æœæˆ‘ä»¬å¯¹ä¸åŒç¼–ç ä½ç½®è§è®¡ç®—çš„self-attentionç»“æœåŠ ä¸Š-100ï¼Œåœ¨Softmaxè®¡ç®—è¿‡ç¨‹ä¸­å°±å¯ä»¥è¾¾åˆ°å½’é›¶çš„æ•ˆæœã€‚

maskçš„å½¢å¼å¦‚ä¸‹å›¾ï¼š

![image-20220226212443486](https://s2.loli.net/2022/02/26/vDizV8fu4e5r6YF.png)

maskä¸ç›¸å¯¹ä½ç½®ç¼–ç æœ‰ç€ä¸€æ ·çš„å½¢å¼ï¼Œ$mask.shape=(num\_windows,M^2,M^2)$

åœ¨ä¸Šé¢ä»‹ç»çš„Blockçš„ä»£ç ä¸­æœ‰è¿™éƒ¨åˆ†çš„ä»£ç å®ç°

```python
# è¿™æ˜¯mask attentionçš„ç›¸å…³ä»£ç 
        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
      # ğŸ‘†åŸä»£ç å°±æ˜¯ä½¿ç”¨-100å»maskæ‰ä¸ç”¨è®¡ç®—çš„éƒ¨åˆ†
```

å‰é¢Window Attentionä»£ç é‡Œå‰å‘ä¼ æ’­ä¹Ÿæœ‰è¿™ä¹ˆä¸€æ®µï¼š

```python
 		if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
```

  ä»¥ä¸Šå…¨éƒ¨å°±æ˜¯Swin Transformeræ¨¡å‹çš„å…¨éƒ¨ä»£ç äº†, ç°åœ¨å†[å›åˆ°å‰é¢](#top)å†çœ‹ä¸€çœ‹ã€‚

## V2

### å‰è¨€

è®ºæ–‡ä¸­æåˆ°Swin Transformer V2[<sup>7</sup>](#swin-transformer-2)å’ŒV1ä¸€æ ·ï¼Œæœ€ç»ˆç›®çš„éƒ½æ˜¯ä¸ºäº†èƒ½å¤Ÿè”åˆå»ºæ¨¡NLPå’ŒCVæ¨¡å‹ã€‚

V2çš„ç›´æ¥ç›®æ ‡æ˜¯å¾—åˆ°ä¸€ä¸ªå¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥åº”ç”¨åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡å¹¶å–å¾—é«˜ç²¾åº¦ã€‚

NLPç›®å‰çš„æ¨¡å‹å‚æ•°å·²ç»è¾¾åˆ°äº†åƒäº¿çº§åˆ«å¹¶ä¸”å‡ºç°äº†åƒBERTè¿™ç§æˆåŠŸçš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥é€‚é…ä¸åŒçš„NLPä»»åŠ¡ï¼›CVç›®å‰æœ€ä¸»è¦çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯æ¨¡å‹è§„æ¨¡ä¸å¤Ÿå¤§ï¼ŒViT-Gå‚æ•°é‡ä¹Ÿåªæœ‰ä¸åˆ°20äº¿ï¼Œå¹¶ä¸”æ‰€æœ‰å¤§è§„æ¨¡çš„è§†è§‰æ¨¡å‹éƒ½åªåº”ç”¨äºå›¾ç‰‡åˆ†ç±»ä»»åŠ¡ã€‚

ä¸ºäº†ç»Ÿä¸€CVå’ŒNLPï¼ŒCVæ¨¡å‹å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼š

- æ¨¡å‹è§„æ¨¡ä¸å¤Ÿå¤§
- é¢„è®­ç»ƒæ¨¡å‹ä¸ä¸‹æ¸¸ä»»åŠ¡å›¾ç‰‡åˆ†è¾¨ç‡ï¼Œçª—å£å¤§å°ä¸é€‚é…

### è§£å†³æ–¹æ³•

å¯¹äºæ¨¡å‹ä¸å¤Ÿå¤§ï¼Œæå‡ºäº†**post-norm and cosine similarity**ï¼›

å¯¹äºæ¨¡å‹ä¸é€‚é…ï¼Œæå‡ºäº†**Continuous relative position bias å’Œ Log-spaced coordinates**

### å¯¹äºæ¨¡å‹ä¸å¤Ÿå¤§

è®ºæ–‡ä¸­æå‡ºäº†SwinV2-G: C=512, layer numbers={2,2,42,2}ï¼Œé€šè¿‡å¢åŠ å¤„ç†æ•°æ®çš„ç»´åº¦(192-->512)å’ŒTransformerå±‚æ•°(18-->42)å¢å¤§æ¨¡å‹è§„æ¨¡ï¼Œä½†æ˜¯ä¼šäº§ç”Ÿä¸€ä¸ªé—®é¢˜ï¼šæ¨¡å‹è®­ç»ƒä¸ç¨³å®šç”šè‡³ä¸èƒ½å®Œæˆè®­ç»ƒã€‚é€šè¿‡å¯¹æ¨¡å‹æ¯ä¸€å±‚è¾“å‡ºçš„åˆ†æï¼Œå‘ç°éšç€æ¨¡å‹å˜æ·±ï¼Œæ²¡å±‚çš„è¾“å‡ºæ˜¯ä¸æ–­å˜å¤§çš„ï¼Œéšç€å±‚æ•°å¢åŠ ï¼Œè¾“å‡ºå€¼ä¸æ–­ç´¯åŠ ï¼Œæ·±å±‚è¾“å‡ºå’Œæµ…å±‚è¾“å‡ºå¹…å€¼å·®å¾ˆå¤šï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®š(Figure 2)ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜å°±æ˜¯è¦ç¨³å®šæ¯ä¸€å±‚çš„è¾“å‡ºï¼Œå…¶å¹…å€¼è¦ç¨³å®šæ‰ä¼šä½¿å¾—è®­ç»ƒè¿‡ç¨‹ç¨³å®šã€‚

![image-20220226221944296](https://s2.loli.net/2022/02/26/nE5mZMVUK3yw91v.png)

ä¸ºäº†ç¨³å®šæ·±å±‚å’Œæµ…å±‚çš„è¾“å‡ºï¼ŒSwin V2 æå‡ºäº†post-norm å’Œ cosine similarity

### post-norm

![image-20220226222040025](https://s2.loli.net/2022/02/26/MYn5XUj9LbtI6wZ.png)

post-norm å°±æ˜¯æŠŠä¹‹å‰é€šç”¨ViTä¸­çš„Transformer blockä¸­çš„Layer Normå±‚ä»Attentionå±‚å‰é¢æŒªåˆ°åé¢ï¼Œè¿™ä¹ˆåšçš„å¥½å¤„å°±æ˜¯è®¡ç®—Attentionä¹‹åä¼šå¯¹è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–æ“ä½œï¼Œç¨³å®šè¾“å‡ºå€¼

### cosine similarity

ViTä¸­Transformer blockè®¡ç®—Attentionæ˜¯é‡‡ç”¨dot(Q,K)çš„æ“ä½œï¼Œåœ¨Swin V2ä¸­å°†å…¶æ›¿æ¢ä¸ºäº†cosine(Q,K)/Ï„ï¼ŒÏ„æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œblockä¹‹é—´ä¸å…±äº«ã€‚cosineè‡ªå¸¦normalizationæ“ä½œï¼Œä¼šè¿›ä¸€æ­¥ç¨³å®šAttentionè¾“å‡ºå€¼

é€šè¿‡post-normå’Œcosine similarityæ“ä½œå°†blockçš„è¾“å‡ºç¨³å®šåœ¨å¯æ¥å—èŒƒå›´å†…, å¸®åŠ©æ¨¡å‹è¿›è¡Œç¨³å®šçš„è®­ç»ƒã€‚

### å¯¹äºæ¨¡å‹ä¸é€‚é…

æ¨¡å‹ä¸é€‚é…ä¸»è¦æ˜¯å›¾ç‰‡åˆ†è¾¨ç‡å’Œçª—å£å¤§å°çš„é—®é¢˜ï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ºäº†èŠ‚çº¦è®­ç»ƒæˆæœ¬ï¼Œé‡‡ç”¨çš„è®­ç»ƒå›¾ç‰‡åˆ†è¾¨ç‡é€šå¸¸æ¯”è¾ƒä½(192/224)ï¼Œä½†æ˜¯å¾ˆå¤šä¸‹æ¸¸ä»»åŠ¡è¦æ±‚çš„å›¾ç‰‡åˆ†è¾¨ç‡è¿œå¤§äºè®­ç»ƒåˆ†è¾¨ç‡çš„(1024/1536)ï¼Œè¿™å°±å¯¼è‡´ä¸€ä¸ªé—®é¢˜ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹è¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ä¸ºäº†é€‚é…è¾“å…¥å›¾ç‰‡åˆ†è¾¨ç‡åŠ¿å¿…è¦æ”¹å˜çª—å£å¤§å°ï¼Œæ”¹å˜çª—å£å¤§å°å°±ä¼šæ”¹å˜patchæ•°é‡ï¼Œæ”¹å˜patchæ•°é‡å°±ä¼šå¯¼è‡´ç›¸å¯¹ä½ç½®ç¼–ç åç½®å‘ç”Ÿå˜åŒ–ã€‚e.g: 8Ã—8 window size å˜ä¸º 16Ã—16 window sizeï¼Œå…¶ç›¸å¯¹ç¼–ç åæ ‡å°±ä»[-7,7] --> [-15,15]ï¼Œå¤šå‡ºæ¥çš„ä½ç½®å¦‚ä½•è®¡ç®—ç›¸å¯¹ä½ç½®ç¼–ç ï¼Ÿ ä¹‹å‰é€šç”¨çš„æ–¹æ³•æ˜¯é‡‡ç”¨äºŒé¡¹ä¸‰æ¬¡å·®å€¼ï¼Œä½†æ˜¯æ•ˆæœæ¬¡ä¼˜ï¼Œä¸å¤Ÿçµæ´»ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSwin V2 æå‡ºäº† Continuous relative position bias å’Œ Log-spaced coordinates

### Continuous relative position bias

V1ä½¿ç”¨äºŒé¡¹ä¸‰æ¬¡å·®å€¼çš„æ–¹æ³•ç”Ÿæˆåç½®Bï¼›è€Œå¦‚Figure1 V2ç¤ºæ„å›¾çš„å·¦ä¸‹è§’çº¢è‰²çš„éƒ¨åˆ†ï¼ŒSwin V2ç›´æ¥ç”¨ä¸¤å±‚MLPï¼ˆMeta networkï¼‰æ¥é€‚åº”ç”Ÿæˆç›¸å¯¹ä½ç½®åç½®ã€‚

### Log-spaced coordinates

patchæ•°é‡å˜åŒ–ä¹‹åéœ€è¦å°†ç›¸å¯¹ä½ç½®ç¼–ç å¤–æ¨ï¼Œåœ¨ä¾‹å­ä¸­çš„å¤–æ¨ç‡æ˜¯1.14Ã—ï¼Œæˆ‘ä»¬å¸Œæœ›çš„æ˜¯é™ä½è¿™ä¸ªæ¯”ç‡ï¼Œå¤–æ¨çš„è¶Šå°è¶Šå¥½ï¼Œæ¯•ç«ŸMeta networkæ²¡è§è¿‡å¤–æ¨ç‰¹åˆ«å¤šçš„è¾“å…¥ï¼Œä¸ºäº†ä¿è¯ç›¸å¯¹ä½ç½®ç¼–ç çš„å‡†ç¡®æ€§ï¼Œéœ€è¦å°†å¤–æ¨æ§åˆ¶åœ¨ä¸€ä¸ªå¯æ¥å—èŒƒå›´å†…ã€‚

Log-spaced coordinateså‡ºç°ï¼Œå°†çº¿æ€§å˜æ¢è½¬æ¢ä¸ºäº†å¯¹æ•°å˜æ¢ï¼š

![image-20220226223004475](https://s2.loli.net/2022/02/26/pYNwrtQIhgfvEqM.png)

![image-20220226223101072](https://s2.loli.net/2022/02/26/vE6I2gyMPHRun1f.png)

![image-20220226223111616](https://s2.loli.net/2022/02/26/RDtxo5Km49FV2Uw.png)





# Reference
<div id="swin-transformer-1"></div>
- [1] [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf)
<div id="vit"/>
- [2] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
<div id="detr"/>
- [3] [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)
<div id="transformer-complexity"/>
- [4] [Transformer/CNN/RNNçš„å¯¹æ¯”ï¼ˆæ—¶é—´å¤æ‚åº¦ï¼Œåºåˆ—æ“ä½œæ•°ï¼Œæœ€å¤§è·¯å¾„é•¿åº¦ï¼‰](https://zhuanlan.zhihu.com/p/264749298)
<div id="pic"/>
- [5] [Swin Transformer è®ºæ–‡è¯¦è§£åŠç¨‹åºè§£è¯»](https://zhuanlan.zhihu.com/p/401661320)
<div id="origin_formula"/>
- [6] [è¶…è¯¦ç»†å›¾è§£Self-Attention](https://zhuanlan.zhihu.com/p/410776234)

  <div id="swin-transformer-2"/>

- [7] [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)

  

- [8] [Swin Transformer V2 è®ºæ–‡è§£æ](https://zhuanlan.zhihu.com/p/445876985)

  

- [9] [å›¾è§£Swin Transformer](https://zhuanlan.zhihu.com/p/367111046)
